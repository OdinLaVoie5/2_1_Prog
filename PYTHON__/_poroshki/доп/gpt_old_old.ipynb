{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d77f420c",
            "metadata": {},
            "source": [
                "I   ПОДГОТОВКА ТЕКСТА  И   ОБУЧЕНИЕ МОДЕЛИ\n",
                "\n",
                "???   В обучении нет явной проверки рифмы между 2 и 4 строкой, только по корпусу.\n",
                "??? Нет разделения на train/test, но для маленьких датасетов это допустимо. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88defcb1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== Шаг 1. Установка пакетов (если нужно) =====\n",
                "#   %pip install transformers datasets accelerate   ((fsspec-2025.7.0))\n",
                " \n",
                "\n",
                "\n",
                "# ===============================================================\n",
                "\"\"\" \n",
                "    Подготовка database, обучение и генерации порошков.  Фильтрация.  \n",
                "\"\"\"\n",
                "\n",
                "# ===== Шаг 3. Датасет =====\n",
                "from datasets import Dataset\n",
                "\n",
                "dataset = Dataset.from_dict({\"text\": unique_poems})\n",
                "# print(dataset)   ??? этот принт не нужен? \n",
                "\n",
                "# ===== Шаг 4. Токенизация =====\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "def tokenize(batch):\n",
                "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
                "# max_length=64  ??? 128? \n",
                "\n",
                "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
                "# print(tokenized)   ??? этот принт не нужен?\n",
                "\n",
                "# ===== Шаг 5. Fine-tuning =====\n",
                "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
                "import torch\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                "\n",
                "fp16 = torch.cuda.is_available() # если есть GPU с поддержкой — можно True\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"poroshki_model\",\n",
                "    overwrite_output_dir=True,\n",
                "    num_train_epochs=3,               # ???  можно увеличить\n",
                "    per_device_train_batch_size=1,    # ноут не потянет больше\n",
                "    save_steps=500,\n",
                "    save_total_limit=2,\n",
                "    logging_dir=\"./logs\",\n",
                "    logging_steps=50,\n",
                "    learning_rate=5e-5,\n",
                "    fp16=fp16                        # если есть GPU с поддержкой — можно True\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized,\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "# ===== Шаг 6. Сохранение =====\n",
                "trainer.save_model(\"poroshki_model\")\n",
                "tokenizer.save_pretrained(\"poroshki_model\")\n",
                "\n",
                "print(\"Модель сохранена в папке: poroshki_model/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "252d2bfd",
            "metadata": {},
            "source": [
                "II  ГЕНЕРАЦИЯ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "85e91bea",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import torch\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "from ruaccent import RUAccent\n",
                "\n",
                "# ======== Модель ======== \n",
                "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
                "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "model.to(device)\n",
                "\n",
                "\n",
                "# ======== Твой фильтр ========\n",
                "\n",
                "# Настройка RUAccent\n",
                "accentizer = RUAccent()\n",
                "accentizer.load(omograph_model_size='turbo3.1',\n",
                "                use_dictionary=True,\n",
                "                tiny_mode=False)\n",
                "\n",
                "VOWELS = \"аеёиоуыэюяАЕЁИОУЫЭЮЯ\"\n",
                "SCHEME = [9, 8, 9, 2]  # схема по слогам\n",
                "\n",
                "\n",
                "def accentize_text(lines: list[str]) -> list[str]:\n",
                "    return [accentizer.process_all(line) for line in lines]\n",
                "\n",
                "\n",
                "def plus_to_upper(lines: list[str]) -> list[str]:\n",
                "    result = []\n",
                "    for line in lines:\n",
                "        chars = list(line)\n",
                "        out = []\n",
                "        skip = False\n",
                "        for i, ch in enumerate(chars):\n",
                "            if skip:\n",
                "                skip = False\n",
                "                continue\n",
                "            if ch == \"+\" and i + 1 < len(chars) and chars[i + 1] in VOWELS:\n",
                "                out.append(chars[i + 1].upper())\n",
                "                skip = True\n",
                "            else:\n",
                "                out.append(ch)\n",
                "        result.append(\"\".join(out))\n",
                "    return result\n",
                "\n",
                "\n",
                "def accent_one_and_yo(lines: list[str]) -> list[str]:\n",
                "    result = []\n",
                "    for line in lines:\n",
                "        chars = list(line)\n",
                "        vowel_positions = [i for i, ch in enumerate(\n",
                "            chars) if ch.lower() in VOWELS]\n",
                "        vowel_order = {pos: idx for idx,\n",
                "                       pos in enumerate(vowel_positions, start=1)}\n",
                "\n",
                "        new_words = []\n",
                "        offset = 0\n",
                "        for word in line.split():\n",
                "            word_chars = list(word)\n",
                "            local_vowels = [i for i, ch in enumerate(\n",
                "                word_chars) if ch.lower() in VOWELS]\n",
                "            if len(local_vowels) == 1:  # односложное слово\n",
                "                i = local_vowels[0]\n",
                "                ch = word_chars[i]\n",
                "                vpos_in_line = offset + i\n",
                "                order = vowel_order.get(vpos_in_line)\n",
                "                if order is not None:\n",
                "                    if order % 2 == 0:  # чётный\n",
                "                        word_chars[i] = ch.upper()\n",
                "                    else:  # нечётный\n",
                "                        word_chars[i] = ch.lower()\n",
                "            elif len(local_vowels) > 1:  # несколько гласных\n",
                "                for i, ch in enumerate(word_chars):\n",
                "                    if ch == \"ё\":\n",
                "                        word_chars[i] = \"Ё\"\n",
                "            new_words.append(''.join(word_chars))\n",
                "            offset += len(word) + 1\n",
                "        result.append(' '.join(new_words))\n",
                "    return result\n",
                "\n",
                "\n",
                "def count_syllables(line: str) -> int:\n",
                "    return sum(1 for ch in line if ch in VOWELS)\n",
                "\n",
                "\n",
                "def line_stresses(line: str) -> list[int]:\n",
                "    stresses = []\n",
                "    syllable = 0\n",
                "    for ch in line:\n",
                "        if ch in VOWELS:\n",
                "            syllable += 1\n",
                "            if ch.isupper():\n",
                "                stresses.append(syllable)\n",
                "    return stresses\n",
                "\n",
                "\n",
                "def analyze_poem_and_accents(accented_lines: list[str]):\n",
                "    scheme_syllables = [count_syllables(line) for line in accented_lines]\n",
                "    scheme_accents = [line_stresses(line) for line in accented_lines]\n",
                "    return scheme_syllables, scheme_accents\n",
                "\n",
                "\n",
                "def check_form(accented_lines: list[str], scheme_ref: list[int]):\n",
                "    SCHEME_SYLLABLES, SCHEME_ACCENTS = analyze_poem_and_accents(\n",
                "        accented_lines)\n",
                "\n",
                "    mismatches = []\n",
                "    for i, syll_count in enumerate(SCHEME_SYLLABLES):\n",
                "        expected = scheme_ref[i] if i < len(scheme_ref) else None\n",
                "        if expected is None:\n",
                "            continue\n",
                "        if syll_count != expected:\n",
                "            mismatches.append({\n",
                "                \"line_no\": i + 1,\n",
                "                \"line\": accented_lines[i],\n",
                "                \"actual\": syll_count,\n",
                "                \"expected\": expected\n",
                "            })\n",
                "\n",
                "    iamb_failures = []\n",
                "    for i, accents in enumerate(SCHEME_ACCENTS):\n",
                "        odd_accents = [a for a in accents if a % 2 == 1]\n",
                "        if odd_accents:\n",
                "            iamb_failures.append({\n",
                "                \"line_no\": i + 1,\n",
                "                \"line\": accented_lines[i],\n",
                "                \"odd_accents\": odd_accents,\n",
                "                \"all_accents\": accents\n",
                "            })\n",
                "\n",
                "    return {\n",
                "        \"SCHEME_SYLLABLES\": SCHEME_SYLLABLES,\n",
                "        \"SCHEME_ACCENTS\": SCHEME_ACCENTS,\n",
                "        \"mismatches\": mismatches,\n",
                "        \"iamb_failures\": iamb_failures\n",
                "    }\n",
                "\n",
                "# ======== Проверка порошка ========\n",
                "def validate_poroshok(poem: str) -> bool:\n",
                "    lines = [l.strip() for l in poem.split(\"\\n\") if l.strip()]\n",
                "    if len(lines) != 4:\n",
                "        return False\n",
                "    try:\n",
                "        accented = accentize_text(lines)\n",
                "        accented_big = plus_to_upper(accented)\n",
                "        accented_all = accent_one_and_yo(accented_big)\n",
                "        result = check_form(accented_all, SCHEME)\n",
                "        if result[\"mismatches\"] or result[\"iamb_failures\"]:\n",
                "            return False\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        print(\"Ошибка акцентуации:\", e)\n",
                "        return False\n",
                "\n",
                "# ======== Генерация ========\n",
                "def generate_poroshki(prompt=\"\"):\n",
                "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "    outputs = model.generate(\n",
                "        input_ids,\n",
                "        max_length=64,\n",
                "        do_sample=True,\n",
                "        top_k=50,\n",
                "        top_p=0.95,\n",
                "        temperature=0.9,\n",
                "        num_return_sequences=10, # количество вариантов ???\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "    texts = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
                "    poroshki = []\n",
                "    for t in texts:\n",
                "        blocks = re.split(r\"\\n\\s*\\n\", t)\n",
                "        for b in blocks:\n",
                "            if validate_poroshok(b):\n",
                "                poroshki.append(b)\n",
                "    return poroshki\n",
                "\n",
                "\n",
                "# ======== Запуск ========\n",
                "good = generate_poroshki(\"\")\n",
                "\n",
                "print(\"Подходящие порошки:\")\n",
                "for p in good:\n",
                "    print(\"—\" * 40)\n",
                "    print(p)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (.venv)",
            "language": "python",
            "name": ".venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
