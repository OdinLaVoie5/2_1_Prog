{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7584cfd6",
   "metadata": {},
   "source": [
    "Просто расстановка ударений (ruaccent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41545d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruaccent import RUAccent\n",
    " \n",
    "accentizer = RUAccent()\n",
    "accentizer.load(omograph_model_size='big_poetry', use_dictionary=True, tiny_mode=False, device=\"CUDA\")\n",
    "\n",
    "text = \"\"\" вахтёр - вахтер\n",
    "вёдро - ведро\n",
    "всё - все\n",
    "вспольё - всполье\n",
    "ёрник - ерник\n",
    "желёзка - железка\n",
    "залёжный - залежный\n",
    "замётка - заметка\n",
    "засолённый - засоленный\n",
    "заторможённый - заторможенный\n",
    "крёстный - крестный\n",
    "лён - лен\n",
    "лётом - летом\n",
    "маркёр - маркер\n",
    "маркёрный - маркерный\n",
    "мётка - метка\n",
    "нёбо - небо\n",
    "падёж - падеж\n",
    "падёжный - падежный\n",
    "перёд - перед\n",
    "письмённый - письменный\n",
    "примётка - приметка\n",
    "совершённое - совершенное\n",
    "солитёр - солитер\n",
    "стартёр - стартер\n",
    "стартёрный - стартерный\n",
    "стрёмить - стремить\n",
    "твёрдо - твердо\n",
    "фён - фен\n",
    "шабёр - шабер\n",
    "\"\"\"\n",
    "print(accentizer.process_all(text)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc44e9",
   "metadata": {},
   "source": [
    "ИНФО\n",
    "__init__ ()\n",
    "_process_accent (text, stress_usages)\n",
    "_process_omographs (text)\n",
    "_process_yo (words, sentence)\n",
    "count_vowels (text)\n",
    "delete_spaces_before_punc (text)\n",
    "extract_entities (data)\n",
    "has_punctuation (text)\n",
    "load (omograph_model_size='turbo2', use_dictionary=False, custom_dict={}, custom_homographs={}, device='CPU', repo='ruaccent/accentuator', workdir=None, tiny_mode=False)\n",
    "process_all (text, skip_regex=None)\n",
    "process_all_internal (text)\n",
    "process_yo (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruaccent import RUAccent\n",
    "import inspect  \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # добавить родительскую папку\n",
    "from DICT_ACC_MY import DICT_ACC_MY\n",
    " \n",
    "accentizer = RUAccent()\n",
    "accentizer.load(omograph_model_size='big_poetry', use_dictionary=True, tiny_mode=False, device=\"CUDA\")\n",
    "# accentizer.load (omograph_model_size='big_poetry', use_dictionary=True, custom_dict=DICT_ACC_MY, custom_homographs={}, device='CPU', tiny_mode=False)\n",
    "\n",
    "text = \"\"\" карасем все вместе идут домой то что  карасем зовется  жопа \"\"\"\n",
    "print(accentizer.process_all(text)) \n",
    "print(accentizer.count_vowels(\"белок\"))\n",
    "print(accentizer.delete_spaces_before_punc(\"белок , торты .\"))\n",
    "print(accentizer.has_punctuation(\"  всё то что ? зовётся  жопа \" ))\n",
    "print(accentizer.process_all_internal(text)) \n",
    "print(accentizer.process_yo(text)) \n",
    "\n",
    "\n",
    "# методы класса RUAccent ??\n",
    "for name, func in inspect.getmembers(accentizer, predicate=inspect.ismethod):\n",
    "    print(name, inspect.signature(func)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f5816",
   "metadata": {},
   "source": [
    "Сведения о внутренних функциях ruaccent   !!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746acb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(RUAccent.__init__))\n",
    "print(inspect.getsource(RUAccent._process_accent))\n",
    "print(inspect.getsource(RUAccent._process_omographs))\n",
    "print(inspect.getsource(RUAccent._process_yo))\n",
    "print(inspect.getsource(RUAccent.count_vowels))\n",
    "print(inspect.getsource(RUAccent.delete_spaces_before_punc))\n",
    "print(inspect.getsource(RUAccent.extract_entities))\n",
    "print(inspect.getsource(RUAccent.has_punctuation))\n",
    "print(inspect.getsource(RUAccent.load))\n",
    "print(inspect.getsource(RUAccent.process_all))\n",
    "print(inspect.getsource(RUAccent.process_all_internal))\n",
    "print(inspect.getsource(RUAccent.process_yo))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fec8a8",
   "metadata": {},
   "source": [
    "ПОДРОБНЕЕ о ruaccent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bcc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он+а пришл+а дом+ой и ув+идела з+амок на двер+и.\n"
     ]
    }
   ],
   "source": [
    "from ruaccent import RUAccent\n",
    "\n",
    "accentizer = RUAccent()\n",
    "accentizer.load(\n",
    "    omograph_model_size=\"turbo3.1\",  # или tiny/turbo/big_poetry\n",
    "    use_dictionary=True,\n",
    "    custom_dict={'замок': 'зам+ок'},  # пример: переопределить ударение (не работает?)\n",
    "    device=\"CUDA\",                  # или \"CPU\"\n",
    "    workdir=None,                   # куда сохранять модели\n",
    "    tiny_mode=False                 # упрощённый режим без словаря\n",
    ")\n",
    "\n",
    "text = \"Она пришла домой и увидела замок на двери.\"\n",
    "accented = accentizer.process_all(text)\n",
    "print(accented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20383744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.cache', 'accent_model.py', 'char_tokenizer.py', 'dictionary', 'koziev', 'nn', 'omograph_model.py', 'ruaccent.py', 'rule_accent_engine.py', 'stress_usage_model.py', 'text_postprocessor.py', 'text_preprocessor.py', 'text_split.py', 'yo_homograph_model.py', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import ruaccent, os\n",
    "print(os.listdir(ruaccent.__path__[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6645",
   "metadata": {},
   "source": [
    "МНОГО ИНФЫ  о ruaccent  (+ onnxruntime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6157f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruaccent version: 1.5.8.3\n",
      "\n",
      "--- Accentizer attributes (select) ---\n",
      "accent_model : <class 'ruaccent.accent_model.AccentModel'>\n",
      "accentuator_paths : <class 'list'>\n",
      "custom_dict : <class 'dict'>\n",
      "koziev_paths : <class 'list'>\n",
      "module_path : <class 'str'>\n",
      "omograph_model : <class 'ruaccent.omograph_model.OmographModel'>\n",
      "omograph_models_paths : <class 'dict'>\n",
      "omographs : <class 'dict'>\n",
      "stress_usage_predictor : <class 'ruaccent.stress_usage_model.StressUsagePredictorModel'>\n",
      "workdir : <class 'str'>\n",
      "yo_homograph_model : <class 'ruaccent.yo_homograph_model.YoHomographModel'>\n",
      "yo_homographs : <class 'dict'>\n",
      "\n",
      "workdir: C:\\Users\\Odins\\new-git\\2_1_Prog\\.venv\\Lib\\site-packages\\ruaccent\n",
      "omograph_models_paths: {'big_poetry': '/nn/nn_omograph/big_poetry', 'medium_poetry': '/nn/nn_omograph/medium_poetry', 'small_poetry': '/nn/nn_omograph/small_poetry', 'turbo': '/nn/nn_omograph/turbo', 'turbo2': '/nn/nn_omograph/turbo2', 'turbo3': '/nn/nn_omograph/turbo3', 'turbo3.1': '/nn/nn_omograph/turbo3.1', 'tiny': '/nn/nn_omograph/tiny', 'tiny2': '/nn/nn_omograph/tiny2', 'tiny2.1': '/nn/nn_omograph/tiny2.1'}\n",
      "expected model path: C:\\Users\\Odins\\new-git\\2_1_Prog\\.venv\\Lib\\site-packages\\ruaccent\\nn/nn_omograph/turbo3.1 exists: True\n",
      "\n",
      "onnxruntime available providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "accentizer session providers: ['CPUExecutionProvider']\n",
      "\n",
      "Original text: Она пришла домой и увидела замок на двери.\n",
      "accentizer.process_all (без предобработки):\n",
      "   Он+а пришл+а дом+ой и ув+идела з+амок на двер+и.\n",
      "\n",
      "After annotate_custom (подставили 'зам+ок'):\n",
      "   Она пришла домой и увидела зам+ок на двери.\n",
      "accentizer.process_all (после подстановки):\n",
      "   Он+а пришл+а дом+ой и ув+идела з+амок на двер+и.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ruaccent\n",
    "from ruaccent import RUAccent\n",
    "\n",
    "print(\"ruaccent version:\", getattr(ruaccent, \"__version__\", \"unknown\"))\n",
    "\n",
    "# --- helper: сохранить case и заменить слова на размеченные ---\n",
    "\n",
    "\n",
    "def annotate_custom(text: str, custom: dict[str, str]) -> str:\n",
    "    if not custom:\n",
    "        return text\n",
    "    # строим паттерн по ключам (lowercase)\n",
    "    keys = sorted(custom.keys(), key=len, reverse=True)\n",
    "    pat = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in keys) + r')\\b',\n",
    "                     flags=re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    def repl(m):\n",
    "        orig = m.group(0)\n",
    "        key = orig.lower()\n",
    "        ann = custom.get(key)\n",
    "        if not ann:\n",
    "            return orig\n",
    "        # сохранить капитализацию первой буквы\n",
    "        if orig and orig[0].isupper():\n",
    "            return ann.capitalize()\n",
    "        return ann\n",
    "\n",
    "    return pat.sub(repl, text)\n",
    "\n",
    "\n",
    "# --- load accentizer (CPU first, чтобы исключить GPU-ошибки) ---\n",
    "accentizer = RUAccent()\n",
    "accentizer.load(\n",
    "    omograph_model_size=\"turbo3.1\",\n",
    "    use_dictionary=True,\n",
    "    custom_dict={'замок': 'зам+ок'},  # пробуем явно\n",
    "    device=\"CPU\",\n",
    "    tiny_mode=False\n",
    ")\n",
    "\n",
    "# --- быстрый debug: посмотреть атрибуты акцентайзера и путь к модели ---\n",
    "print(\"\\n--- Accentizer attributes (select) ---\")\n",
    "for k, v in sorted(accentizer.__dict__.items()):\n",
    "    if any(sub in k.lower() for sub in (\"dict\", \"omograph\", \"workdir\", \"model\", \"path\")):\n",
    "        print(k, \":\", type(v))\n",
    "\n",
    "# попытка найти путь до onnx модели\n",
    "omap = getattr(accentizer, \"omograph_models_paths\", None)\n",
    "wd = getattr(accentizer, \"workdir\", None)\n",
    "print(\"\\nworkdir:\", wd)\n",
    "print(\"omograph_models_paths:\", omap)\n",
    "\n",
    "if wd and omap and \"turbo3.1\" in omap:\n",
    "    rel = omap[\"turbo3.1\"]\n",
    "    candidate = os.path.join(wd, rel.lstrip(\"/\\\\\"))\n",
    "    print(\"expected model path:\", candidate,\n",
    "          \"exists:\", os.path.exists(candidate))\n",
    "\n",
    "# onnxruntime провайдеры (показывает, доступен ли CUDAExecutionProvider)\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    print(\"\\nonnxruntime available providers:\", ort.get_available_providers())\n",
    "    sess = getattr(getattr(accentizer, \"omograph_model\",\n",
    "                   object()), \"session\", None)\n",
    "    if sess:\n",
    "        try:\n",
    "            print(\"accentizer session providers:\", sess.get_providers())\n",
    "        except Exception as e:\n",
    "            print(\"can't read session providers:\", e)\n",
    "except Exception as e:\n",
    "    print(\"\\nonnxruntime import failed or not installed:\", e)\n",
    "\n",
    "# --- проверка custom dict в действии ---\n",
    "text = \"Она пришла домой и увидела замок на двери.\"\n",
    "print(\"\\nOriginal text:\", text)\n",
    "print(\"accentizer.process_all (без предобработки):\")\n",
    "print(\"  \", accentizer.process_all(text))\n",
    "\n",
    "text_annot = annotate_custom(text, {'замок': 'зам+ок'})\n",
    "print(\"\\nAfter annotate_custom (подставили 'зам+ок'):\")\n",
    "print(\"  \", text_annot)\n",
    "print(\"accentizer.process_all (после подстановки):\")\n",
    "print(\"  \", accentizer.process_all(text_annot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62cf64b",
   "metadata": {},
   "source": [
    "проверка откуда пакет и его версия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3efb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ruaccent\n",
    "\n",
    "print(ruaccent.__file__)\n",
    "print(sys.executable)\n",
    "\n",
    "print(ruaccent.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a2d00",
   "metadata": {},
   "source": [
    "инфо коротко"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37354ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(\n",
    "    0) if torch.cuda.is_available() else \"none\")\n",
    "print(\"onnxruntime providers:\", ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58695d6",
   "metadata": {},
   "source": [
    "============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4875c46",
   "metadata": {},
   "source": [
    "другой способ сравнения ритмов \n",
    "(нужна коррекция - ударения однослоговых   и  \"ё\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423132af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ruaccent import RUAccent\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Настройка RUAccent\n",
    "# -----------------------------\n",
    "accentizer = RUAccent()\n",
    "accentizer.load(omograph_model_size=\"turbo3.1\",\n",
    "                use_dictionary=True, tiny_mode=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Константы\n",
    "# -----------------------------\n",
    "VOWELS = \"аеёиоуыэюяАЕЁИОУЫЭЮЯ\"\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Анализ ритма по слогам\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def analyze_rhythm(accented_line: str) -> str:\n",
    "    \"\"\"\n",
    "    Получает строку с ударениями (ruaccent, + перед ударной гласной),\n",
    "    возвращает схему 0/1 по слогам.\n",
    "    \"\"\"\n",
    "    rhythm = []\n",
    "    i = 0\n",
    "    while i < len(accented_line):\n",
    "        ch = accented_line[i]\n",
    "        if ch == \"+\" and i + 1 < len(accented_line) and accented_line[i+1] in VOWELS:\n",
    "            rhythm.append(\"1\")   # ударная гласная\n",
    "            i += 1               # пропускаем саму гласную\n",
    "        elif ch in VOWELS:\n",
    "            rhythm.append(\"0\")   # безударная гласная\n",
    "        i += 1\n",
    "    return \"\".join(rhythm)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Проверка схемы\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def check_rhythm_scheme(rhythm: str, scheme: str) -> bool:\n",
    "    return rhythm == scheme\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Полный анализ стихотворения\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def _analyze_poem(poem: str, scheme: str) -> list[tuple[str, str, str, bool]]:\n",
    "    \"\"\"\n",
    "    Анализирует стихотворение.\n",
    "    Возвращает список кортежей:\n",
    "    (оригинал строки, строка с ударениями, ритм, совпадает ли со схемой).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for line in poem.splitlines():\n",
    "        clean_line = line.strip()\n",
    "        if not clean_line:\n",
    "            continue\n",
    "        accented_line = accentizer.process_all(clean_line)\n",
    "        rhythm = analyze_rhythm(accented_line)\n",
    "        matches = check_rhythm_scheme(rhythm, scheme)\n",
    "        results.append((clean_line, accented_line, rhythm, matches))\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Пример использования\n",
    "# -----------------------------\n",
    "SCHEME = \"0101\"  # для примера\n",
    "\n",
    "poem = \"\"\"привет салют \n",
    "на перекрёстке двух дорог\n",
    "я ждал вас там четыре года\n",
    "и вот вы здесь\"\"\"\n",
    "\n",
    "results = _analyze_poem(poem, SCHEME)\n",
    "\n",
    "for line, accented, rhythm, matches in results:\n",
    "    print(f\"Строка: {line}\")\n",
    "    print(f\"Ударения: {accented}\")\n",
    "    print(f\"Ритм: {rhythm}\")\n",
    "    print(f\"Соответствует схеме: {matches}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a54fd",
   "metadata": {},
   "source": [
    "====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddcfad",
   "metadata": {},
   "source": [
    "дб     УДАРЕНИЯ (ОБХОД ruaccent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from ruaccent import RUAccent\n",
    "\n",
    "\n",
    "def try_annotate_custom(accentizer, text, custom_dict):\n",
    "    \"\"\"Попытаться вызвать annotate_custom в известных вариантах сигнатуры.\"\"\"\n",
    "    f = getattr(accentizer, \"annotate_custom\", None)\n",
    "    if f is None:\n",
    "        return None\n",
    "    # пробуем разные варианты вызова — сигнатура у разных версий может отличаться\n",
    "    try:\n",
    "        return f(text, custom_dict)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return f(custom_dict)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return f(text)\n",
    "            except Exception:\n",
    "                return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def apply_custom_postprocess(processed_text: str, custom_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    В processed_text (строка, выход accentizer.process_all) находит слова,\n",
    "    соответствующие ключам custom_dict (игнорируя '+' и регистр), и\n",
    "    заменяет их на значение из custom_dict. Сохраняет первую букву в верхнем регистре,\n",
    "    если в исходном токене была заглавная.\n",
    "    custom_dict: {'замок': 'зам+ок', ...}\n",
    "    \"\"\"\n",
    "    # нормализуем ключи: удалить не-буквы, привести к lower\n",
    "    norm_map = {regex.sub(r'[^\\p{L}]', '', k).lower(): v for k, v in custom_dict.items()}\n",
    "\n",
    "    # разобьём processed_text на токены: слова (буквы и +) и всё остальное\n",
    "    tokens = regex.findall(r'(?:\\+?[\\p{L}\\+]+|[^\\p{L}\\+]+)', processed_text)\n",
    "\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if regex.fullmatch(r'[\\p{L}\\+]+', tok):\n",
    "            # слово-подобный токен: убираем '+' и нормализуем\n",
    "            stripped = tok.replace('+', '')\n",
    "            norm = regex.sub(r'[^\\p{L}]', '', stripped).lower()\n",
    "            if norm in norm_map:\n",
    "                replacement = norm_map[norm]\n",
    "                # сохранить капитализацию первой буквы, если в исходном токене первая буква была заглавной\n",
    "                first_orig_letter = next(\n",
    "                    (ch for ch in stripped if ch.isalpha()), None)\n",
    "                if first_orig_letter and first_orig_letter.isupper():\n",
    "                    # найти индекс первой буквы в replacement и сделать её заглавной\n",
    "                    rep_list = list(replacement)\n",
    "                    for i, ch in enumerate(rep_list):\n",
    "                        if ch.isalpha():\n",
    "                            rep_list[i] = rep_list[i].upper()\n",
    "                            break\n",
    "                    replacement = ''.join(rep_list)\n",
    "                out.append(replacement)\n",
    "                continue\n",
    "        out.append(tok)\n",
    "    return ''.join(out)\n",
    "\n",
    "\n",
    "def accent_with_custom(accentizer: RUAccent, text: str, custom_dict: dict, try_annotate=True) -> str:\n",
    "    \"\"\"\n",
    "    Главная обёртка: пытается annotate_custom, запускает process_all,\n",
    "    затем принудительно применяет custom_dict постфактум.\n",
    "    \"\"\"\n",
    "    # обновим accentizer.custom_dict (если оно есть) — иногда помогает\n",
    "    try:\n",
    "        cd = getattr(accentizer, \"custom_dict\", None)\n",
    "        if isinstance(cd, dict):\n",
    "            cd.update(custom_dict)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    annotated_text = None\n",
    "    if try_annotate:\n",
    "        annotated_text = try_annotate_custom(accentizer, text, custom_dict)\n",
    "\n",
    "    if annotated_text is not None:\n",
    "        processed = accentizer.process_all(annotated_text)\n",
    "    else:\n",
    "        processed = accentizer.process_all(text)\n",
    "\n",
    "    final = apply_custom_postprocess(processed, custom_dict)\n",
    "    return final\n",
    "\n",
    "\n",
    "# --------------------- пример использования ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    accentizer = RUAccent()\n",
    "    accentizer.load(\n",
    "        omograph_model_size=\"turbo3.1\",\n",
    "        use_dictionary=True,\n",
    "        custom_dict={},   # можно передать сразу, но wrapper обновит тоже   НЕ РАБОТАЕТ ???\n",
    "        device=\"CUDA\",     # сначала тестируем на CPU; чтобы использовать CUDA — см. выше\n",
    "        tiny_mode=False, \n",
    "        # workdir=None   где хранятся модели (по умолчанию '~/.ruaccent'\n",
    "    )\n",
    "\n",
    "    text = \"Она пришла домой и увидела замок на двери.\"\n",
    "    custom = {\"замок\": \"зам+ок\"}   # ключи — в простой форме, значения — с '+'\n",
    "    out = accent_with_custom(accentizer, text, custom)\n",
    "    print(out)\n",
    "    print(accentizer.process_all(text))  # для сравнения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a393b",
   "metadata": {},
   "source": [
    "дб   ЭТО В _accentize.py   (ещё см GPT)    ОБЁРТКИ НА ruaccent (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from ruaccent import RUAccent\n",
    "\n",
    "# какая то фигня - выкинул\n",
    "def try_annotate_custom(accentizer, text, custom_dict):\n",
    "    f = getattr(accentizer, \"annotate_custom\", None)\n",
    "    if f is None:\n",
    "        return None\n",
    "    try:\n",
    "        return f(text, custom_dict)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return f(custom_dict)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return f(text)\n",
    "            except Exception:\n",
    "                return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "#  ? сделать   сохранить от изменения \"е\" и \"ё\" (или есть отключение ёфикатора?)\n",
    "def apply_custom_postprocess(processed_text: str, custom_dict: dict) -> str:\n",
    "    norm_map = {regex.sub(r'[^\\p{L}]', '', k).lower(): v for k, v in custom_dict.items()}\n",
    "    tokens = regex.findall(r'(?:\\+?[\\p{L}\\+]+|[^\\p{L}\\+]+)', processed_text)\n",
    "\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if regex.fullmatch(r'[\\p{L}\\+]+', tok):\n",
    "            stripped = tok.replace('+', '')\n",
    "            norm = regex.sub(r'[^\\p{L}]', '', stripped).lower()\n",
    "            if norm in norm_map:\n",
    "                replacement = norm_map[norm]\n",
    "                # сохранить заглавную первую букву\n",
    "                first_orig_letter = next((ch for ch in stripped if ch.isalpha()), None)\n",
    "                if first_orig_letter and first_orig_letter.isupper():\n",
    "                    rep_list = list(replacement)\n",
    "                    for i, ch in enumerate(rep_list):\n",
    "                        if ch.isalpha():\n",
    "                            rep_list[i] = rep_list[i].upper()\n",
    "                            break\n",
    "                    replacement = ''.join(rep_list)\n",
    "                out.append(replacement)\n",
    "                continue\n",
    "        out.append(tok)\n",
    "    return ''.join(out)\n",
    "\n",
    "\n",
    "def accentize(\n",
    "    text: str,\n",
    "    accentizer: RUAccent = None,\n",
    "    use_manual=True,          # CUSTOM_ACCENT\n",
    "    custom_dict=None,         # CUSTOM_DICT\n",
    "    try_annotate=True,        # пробовать annotate_custom (если есть у самого accentizer)\n",
    "    bypass_ruaccent=False,    # полный обход ruaccent\n",
    "    debug=False               # печатать шаги\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Универсальная обёртка над ruaccent.\n",
    "    text            — входной текст\n",
    "    accentizer      — экземпляр RUAccent (или None → bypass)\n",
    "    use_manual      — True: сохранять слова с '+'\n",
    "    custom_dict     — словарь {'замок': 'зам+ок'}\n",
    "    try_annotate    — пробовать accentizer.annotate_custom (если есть)\n",
    "    bypass_ruaccent — True: игнорировать ruaccent, работать только словарём и '+'\n",
    "    debug           — печатать выбранный режим\n",
    "    \"\"\"\n",
    "    if custom_dict is None:\n",
    "        custom_dict = {}\n",
    "\n",
    "    # авто-bypass если accentizer не передан\n",
    "    if accentizer is None:\n",
    "        bypass_ruaccent = True\n",
    "        if debug:\n",
    "            print(\"[DEBUG] accentizer=None → bypass_ruaccent включён\")\n",
    "\n",
    "    # --- 1. Сохраняем ручные акценты ---\n",
    "    manual_accents = []\n",
    "    words = text.split()\n",
    "    clean_text = text\n",
    "    if use_manual:\n",
    "        manual_accents = [(i, w) for i, w in enumerate(words) if \"+\" in w]\n",
    "        clean_text = clean_text.replace(\"+\", \"\")\n",
    "        if debug and manual_accents:\n",
    "            print(f\"[DEBUG] Найдено {len(manual_accents)} слов с ручными акцентами\")\n",
    "\n",
    "    # --- 2. Основная логика ---\n",
    "    if bypass_ruaccent:\n",
    "        processed = clean_text\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Пропускаем ruaccent, работаем только словарём/ручными акцентами\")\n",
    "    else:\n",
    "        annotated_text = None\n",
    "        if custom_dict and try_annotate:\n",
    "            annotated_text = try_annotate_custom(accentizer, clean_text, custom_dict)\n",
    "            if debug and annotated_text is not None:\n",
    "                print(\"[DEBUG] Успешно вызван annotate_custom\")\n",
    "\n",
    "        if annotated_text is not None:\n",
    "            processed = accentizer.process_all(annotated_text)\n",
    "            if debug:\n",
    "                print(\"[DEBUG] Запуск process_all на annotate_custom\")\n",
    "        else:\n",
    "            processed = accentizer.process_all(clean_text)\n",
    "            if debug:\n",
    "                print(\"[DEBUG] Запуск process_all напрямую\")\n",
    "\n",
    "    # --- 3. Восстанавливаем ручные акценты ---\n",
    "    if manual_accents:\n",
    "        processed_words = processed.split()\n",
    "        for idx, manual_word in manual_accents:\n",
    "            if idx < len(processed_words):\n",
    "                processed_words[idx] = manual_word\n",
    "        processed = \" \".join(processed_words)\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Ручные акценты восстановлены\")\n",
    "\n",
    "    # --- 4. Применяем custom_dict ---\n",
    "    if custom_dict:\n",
    "        processed = apply_custom_postprocess(processed, custom_dict)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Применён custom_dict (размер={len(custom_dict)})\")\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "# ---------------- пример ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    accentizer = RUAccent()\n",
    "    accentizer.load(\n",
    "        omograph_model_size=\"turbo2\",\n",
    "        use_dictionary=True,\n",
    "        device=\"CPU\",\n",
    "    )\n",
    "\n",
    "    txt = \"б+езрукий паслён кричит замок на двери\"\n",
    "    custom = {\"замок\": \"зам+ок\"}\n",
    "\n",
    "    print(\"ruaccent + словарь:\")\n",
    "    print(accentize(txt, accentizer, use_manual=True, custom_dict=custom, debug=True))\n",
    "    print(\"\\nтолько словарь (bypass):\")\n",
    "    print(accentize(txt, custom_dict=custom, debug=True))  # accentizer=None → bypass\n",
    "    print(\"\\nтолько ruaccent:\") \n",
    "    print(accentize(txt, accentizer, use_manual=False, debug=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76109b2c",
   "metadata": {},
   "source": [
    "дб   2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from ruaccent import RUAccent\n",
    "\n",
    "\n",
    "def try_annotate_custom(accentizer, text, custom_dict):\n",
    "    \"\"\"Универсальный вызов accentizer.annotate_custom (если он есть).\"\"\"\n",
    "    f = getattr(accentizer, \"annotate_custom\", None)\n",
    "    if f is None:\n",
    "        return None\n",
    "    try:\n",
    "        return f(text, custom_dict)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return f(custom_dict)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return f(text)\n",
    "            except Exception:\n",
    "                return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def apply_custom_postprocess(processed_text: str, custom_dict: dict) -> str:\n",
    "    \"\"\"Постобработка словарём: учитывает регистр, чистит символы.\"\"\"\n",
    "    norm_map = {regex.sub(r\"[^\\p{L}]\", \"\", k).lower(): v for k, v in custom_dict.items()}\n",
    "    tokens = regex.findall(r\"(?:\\+?[\\p{L}\\+]+|[^\\p{L}\\+]+)\", processed_text)\n",
    "\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if regex.fullmatch(r\"[\\p{L}\\+]+\", tok):\n",
    "            stripped = tok.replace(\"+\", \"\")\n",
    "            norm = regex.sub(r\"[^\\p{L}]\", \"\", stripped).lower()\n",
    "            if norm in norm_map:\n",
    "                replacement = norm_map[norm]\n",
    "                # сохраняем заглавную первую букву\n",
    "                first_orig_letter = next((ch for ch in stripped if ch.isalpha()), None)\n",
    "                if first_orig_letter and first_orig_letter.isupper():\n",
    "                    rep_list = list(replacement)\n",
    "                    for i, ch in enumerate(rep_list):\n",
    "                        if ch.isalpha():\n",
    "                            rep_list[i] = rep_list[i].upper()\n",
    "                            break\n",
    "                    replacement = \"\".join(rep_list)\n",
    "                out.append(replacement)\n",
    "                continue\n",
    "        out.append(tok)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def load_default_dict():\n",
    "    \"\"\"Простейший дефолтный словарь — заглушка.\"\"\"\n",
    "    return {\"замок\": \"зам+ок\", \"атлас\": \"атл+ас\"}\n",
    "\n",
    "\n",
    "def accentize(\n",
    "    text: str,\n",
    "    accentizer: RUAccent = None,\n",
    "    use_manual=True,          # сохранять слова с '+'\n",
    "    custom_dict=None,         # None → дефолтный словарь, {} → словарь отключен\n",
    "    try_annotate=True,        # пробовать accentizer.annotate_custom\n",
    "    bypass_ruaccent=False,    # полный обход ruaccent\n",
    "    debug=False,              # печатать шаги\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Универсальная обёртка над ruaccent.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Разбираемся со словарём ---\n",
    "    if custom_dict is None:\n",
    "        custom_dict = load_default_dict()\n",
    "        if debug:\n",
    "            print(\"[DEBUG] custom_dict=None → загружен дефолтный словарь\")\n",
    "    elif custom_dict == {}:\n",
    "        custom_dict = None\n",
    "        if debug:\n",
    "            print(\"[DEBUG] custom_dict={} → словарь отключён\")\n",
    "\n",
    "    # --- 2. Авто-инициализация accentizer ---\n",
    "    if not bypass_ruaccent and accentizer is None:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] accentizer=None → создаём новый RUAccent()\")\n",
    "        accentizer = RUAccent()\n",
    "        accentizer.load(omograph_model_size=\"turbo2\", use_dictionary=True, device=\"CPU\")\n",
    "\n",
    "    # --- 3. Сохраняем ручные акценты ---\n",
    "    manual_accents = []\n",
    "    words = text.split()\n",
    "    clean_text = text\n",
    "    if use_manual:\n",
    "        manual_accents = [(i, w) for i, w in enumerate(words) if \"+\" in w]\n",
    "        clean_text = clean_text.replace(\"+\", \"\")\n",
    "        if debug and manual_accents:\n",
    "            print(f\"[DEBUG] Найдено {len(manual_accents)} слов с ручными акцентами\")\n",
    "\n",
    "    # --- 4. Основная логика ---\n",
    "    if bypass_ruaccent:\n",
    "        processed = clean_text\n",
    "        if debug:\n",
    "            print(\"[DEBUG] bypass_ruaccent=True → работаем только словарём/ручными акцентами\")\n",
    "    else:\n",
    "        annotated_text = None\n",
    "        if custom_dict and try_annotate:\n",
    "            annotated_text = try_annotate_custom(accentizer, clean_text, custom_dict)\n",
    "            if debug and annotated_text is not None:\n",
    "                print(\"[DEBUG] Успешно вызван annotate_custom\")\n",
    "\n",
    "        if annotated_text is not None:\n",
    "            processed = accentizer.process_all(annotated_text)\n",
    "            if debug:\n",
    "                print(\"[DEBUG] Запуск process_all на annotate_custom\")\n",
    "        else:\n",
    "            processed = accentizer.process_all(clean_text)\n",
    "            if debug:\n",
    "                print(\"[DEBUG] Запуск process_all напрямую\")\n",
    "\n",
    "    # --- 5. Восстанавливаем ручные акценты ---\n",
    "    if manual_accents:\n",
    "        processed_words = processed.split()\n",
    "        for idx, manual_word in manual_accents:\n",
    "            if idx < len(processed_words):\n",
    "                processed_words[idx] = manual_word\n",
    "        processed = \" \".join(processed_words)\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Ручные акценты восстановлены\")\n",
    "\n",
    "    # --- 6. Применяем custom_dict ---\n",
    "    if custom_dict:\n",
    "        processed = apply_custom_postprocess(processed, custom_dict)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Применён custom_dict (размер={len(custom_dict)})\")\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "# ---------------- пример ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    txt = \"б+езрукий паслён кричит замок на двери\"\n",
    "\n",
    "    print(\"ruaccent + словарь:\")\n",
    "    print(accentize(txt, use_manual=True, debug=True))\n",
    "\n",
    "    print(\"\\nтолько словарь (bypass):\")\n",
    "    print(accentize(txt, custom_dict={\"замок\": \"зам+ок\"}, bypass_ruaccent=True, debug=True))\n",
    "\n",
    "    print(\"\\nтолько ruaccent:\")\n",
    "    print(accentize(txt, custom_dict={}, use_manual=False, debug=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef86212",
   "metadata": {},
   "source": [
    "=============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68833ab9",
   "metadata": {},
   "source": [
    "дб  попытка проставить ударения в составных словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e17d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruaccent import RUAccent\n",
    "\n",
    "accentizer = RUAccent()\n",
    "accentizer.load(omograph_model_size='big_poetry',\n",
    "                use_dictionary=True, tiny_mode=False) #  tiny_mode лучше иногда ?\n",
    "\n",
    "def complex_words_postprocess(processed_text: str, accentizer: RUAccent) :\n",
    "    \"\"\"\n",
    "    Обрабатывает слова, которые не получили ударений:\n",
    "      1) пробуем «подрезку» слова от начала (cut-rule),\n",
    "      2) если всё ещё нет — ставим '+' перед 'ё' (yo-rule).\n",
    "    Возвращает (обновлённый текст, список изменений).\n",
    "    \"\"\"\n",
    "    vowels = \"аеёиоуыэюяАЕЁИОУЫЭЮЯ\"\n",
    "    words = processed_text.split()\n",
    "    post_changes = []\n",
    "    saved = []\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "        # пропускаем: односложные или уже с \"+\"\n",
    "        if sum(ch in vowels for ch in w) <= 1 or \"+\" in w:\n",
    "            continue\n",
    "\n",
    "        # --- 1. cut-rule ---\n",
    "        applied = False\n",
    "        for j in range(min(7, max(0, len(w) - 3))): # ?? не оставлять коротких обрезков-слов и не брать много с начала\n",
    "            cut_w = w[j:]\n",
    "            acc_cut_w = accentizer.process_all(cut_w)\n",
    "            if \"+\" in acc_cut_w:\n",
    "                new_w = w[:j] + acc_cut_w.replace(\"+\", \"?\")\n",
    "                post_changes.append((i, w, new_w, \"cut\", acc_cut_w))\n",
    "                saved.insert(0, new_w) \n",
    "                applied = True\n",
    "                break\n",
    "\n",
    "        if applied:\n",
    "            continue\n",
    "\n",
    "        # --- 2. ё-rule ---\n",
    "        if \"ё\" in w:\n",
    "            new_w = w.replace(\"ё\", \"?ё\")     # отмечаем все ё\n",
    "            post_changes.append((i, w, new_w, \"yo\"))\n",
    "            saved.insert(0, new_w) \n",
    "\n",
    "        else: saved.append(w)     # слова оставшиеся без \"+\"\n",
    "\n",
    "    return post_changes, saved\n",
    "\n",
    "\n",
    "text = accentizer.process_all(\"\"\"сёгун  трёхколёсный четырёхколёсный четырёхкамерный\n",
    "вертолётостроение самолётостроение сёрфингиста \"\"\")\n",
    "print(text)\n",
    "print(complex_words_postprocess(text, accentizer)[0]) \n",
    "print(complex_words_postprocess(text, accentizer)[1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaaa100",
   "metadata": {},
   "source": [
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df3533",
   "metadata": {},
   "source": [
    "(от ии. не проверен)   Делим текст на куски (чанки) для nn. Безопасный вызов accentizer.process_all.  (было для accentize.py)   \n",
    "+ см   py библиотечка razdel (для токенизации русских слов)  ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eaeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  (от ии. не проверен)   Делим текст на куски (чанки) для nn. Безопасный вызов accentizer.process_all.\n",
    "def _safe_process(accentizer, text: str, max_len: int = 400):\n",
    "    \"\"\"\n",
    "    Аккуратно обрабатывает текст, деля его на чанки:\n",
    "    1. Собираем несколько абзацев вместе, если помещаются в max_len.\n",
    "    2. Если абзац длинный — режем его по строкам.\n",
    "    3. Если и строка длинная — делим по словам.\n",
    "    \"\"\"\n",
    "\n",
    "    def chunk_text_by_lines(text: str, max_chars: int = 400) -> list[str]:\n",
    "        \"\"\"Делим текст по абзацам, объединяя их до max_chars, но абзацы не рвём при накоплении.\n",
    "        Если абзац сам длиннее лимита — делим его по строкам, строки рвём только если они длиннее лимита (и если строка — одно длинное слово, делим по max_chars).\"\"\"\n",
    "        import re\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        chunks = []\n",
    "        buf = []\n",
    "        buf_len = 0\n",
    "\n",
    "        def split_paragraph(para: str, max_chars: int, max_word_len: int = 40) -> list[str]:\n",
    "            lines = para.splitlines()\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_len = 0\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                words = line.split()\n",
    "                # если строка — одно длинное слово (нет пробелов)\n",
    "                if len(words) == 1 and len(words[0]) > max_chars:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(\"\\n\".join(current_chunk))\n",
    "                        current_chunk = []\n",
    "                        current_len = 0\n",
    "                    # разбиваем длинное слово на куски по max_word_len\n",
    "                    for i in range(0, len(line), max_word_len):\n",
    "                        part = line[i:i+max_word_len]\n",
    "                        chunks.append(part)\n",
    "                    continue\n",
    "\n",
    "                if len(line) > max_chars:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(\"\\n\".join(current_chunk))\n",
    "                        current_chunk = []\n",
    "                        current_len = 0\n",
    "                    buf = []\n",
    "                    buf_len = 0\n",
    "                    for w in words:\n",
    "                        if buf_len + len(w) + 1 > max_chars:\n",
    "                            chunks.append(\" \".join(buf))\n",
    "                            buf = [w]\n",
    "                            buf_len = len(w)\n",
    "                        else:\n",
    "                            buf.append(w)\n",
    "                            buf_len += len(w) + 1\n",
    "                    if buf:\n",
    "                        chunks.append(\" \".join(buf))\n",
    "                else:\n",
    "                    if current_len + len(line) + 1 > max_chars:\n",
    "                        chunks.append(\"\\n\".join(current_chunk))\n",
    "                        current_chunk = [line]\n",
    "                        current_len = len(line)\n",
    "                    else:\n",
    "                        current_chunk.append(line)\n",
    "                        current_len += len(line) + 1\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(\"\\n\".join(current_chunk))\n",
    "            return [chunk for chunk in chunks if chunk.strip()]\n",
    "\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "\n",
    "            if len(para) > max_chars:\n",
    "                if buf:\n",
    "                    chunks.append(\"\\n\\n\".join(buf))\n",
    "                    buf, buf_len = [], 0\n",
    "                chunks.extend(split_paragraph(para, max_chars, max_word_len=0))\n",
    "                continue\n",
    "\n",
    "            if buf_len == 0:\n",
    "                buf.append(para)\n",
    "                buf_len = len(para) + 2\n",
    "            elif buf_len + len(para) + 2 <= max_chars:\n",
    "                buf.append(para)\n",
    "                buf_len += len(para) + 2\n",
    "            else:\n",
    "                chunks.append(\"\\n\\n\".join(buf))\n",
    "                buf = [para]\n",
    "                buf_len = len(para) + 2\n",
    "\n",
    "        if buf:\n",
    "            chunks.append(\"\\n\\n\".join(buf))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    # --- основная логика safe_process ---\n",
    "    result = []\n",
    "    for chunk in chunk_text_by_lines(text, max_chars=max_len):\n",
    "        print(\"\\n--- ЧАНК ---\\n\", chunk, \"\\n\")  # <-- вывод чанка\n",
    "        processed = accentizer.process_all(chunk)\n",
    "        result.append(processed)\n",
    "\n",
    "    return \"\\n\\n\".join(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f6290",
   "metadata": {},
   "source": [
    "======================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338814f",
   "metadata": {},
   "source": [
    "!!!!!  Патчнуть класс.      ОТКЛЮЧИТЬ ЁФИКАЦИЮ   !!!\n",
    "У RUAccent внутри вызывается метод _process_yo(). Можно просто переопределить его. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruaccent import RUAccent\n",
    "\n",
    "class RUAccentNoYo(RUAccent):\n",
    "    def _process_yo(self, words, sentence):\n",
    "        return words  # ничего не делаем\n",
    "\n",
    "accentizer = RUAccentNoYo()\n",
    "accentizer.load(omograph_model_size='big_poetry',\n",
    "                use_dictionary=True, tiny_mode=False, device=\"CPU\")\n",
    "\n",
    "text = \"когда нибудь ты все увидишь\"\n",
    "\n",
    "print(accentizer.process_all(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eee96b",
   "metadata": {},
   "source": [
    "!!!  Чуть сложнее чем ^  (подменяет оставляя имя RUAccent)\n",
    "(ещё сложнее (с контекст-отключением и с monkeypatch)  см 1_эксперим\\my_ruaccent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04634a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruaccent import RUAccent as _RUAccent\n",
    "\n",
    "class RUAccent(_RUAccent):\n",
    "    def __init__(self, *args, disable_yo: bool = False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._disable_yo = bool(disable_yo)\n",
    " \n",
    "    def disable_yo(self, state: bool = True):\n",
    "        \"\"\"Включить/выключить ё-фиксацию на лету.\"\"\"\n",
    "        self._disable_yo = bool(state)\n",
    "\n",
    "    def _process_yo(self, words, sentence):\n",
    "        # если выключено — не делаем замен; иначе передаём реализацию родителю\n",
    "        if self._disable_yo:\n",
    "            return words\n",
    "        return super()._process_yo(words, sentence)\n",
    "    \n",
    "\"\"\" далее в др. файле\n",
    "from my_ruaccent import RUAccent\n",
    "\n",
    "acc = RUAccent()\n",
    "acc.load()\n",
    "\n",
    "text = \"Все хорошо`\"\n",
    "\n",
    "print(acc.process_all(text))        # с ёфикацией (по умолчанию)\n",
    "acc.disable_yo(True)                #     отключаем\n",
    "print(acc.process_all(text))        # без ёфикации\n",
    "acc.disable_yo(False)               #     включаем обратно\n",
    "print(acc.process_all(text))        # снова с ёфикацией\n",
    "\n",
    "with acc.no_yo():\n",
    "    print(acc.process_all(text))  # временно без ёфикации\n",
    "print(acc.process_all(text))   # снова с ёфикацией\n",
    "\n",
    "\n",
    "acc2 = RUAccent(disable_yo=True)  # сразу экземпляр с отключённой ёфикацией \n",
    "acc2.load()\n",
    "print(acc2.process_all(\"Все хорошо\")) # никаких новых 'ё'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af621b6",
   "metadata": {},
   "source": [
    "====================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
