{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "54052b22",
            "metadata": {},
            "source": [
                "Простое удаление и восстановление переносов строки (карта по символам)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "acfa6b0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_linebreak_map(text: str):\n",
                "    \"\"\"Возвращает список позиций переносов строк\"\"\"\n",
                "    return [i for i, ch in enumerate(text) if ch == \"\\n\"]\n",
                "\n",
                "def replacelinebreaks_with_spaces(text: str):\n",
                "    \"\"\"Заменяет \\n на пробелы и возвращает карту\"\"\"\n",
                "    lb_map = get_linebreak_map(text)\n",
                "    return text.replace(\"\\n\", \" \"), lb_map\n",
                "\n",
                "def restorelinebreaks(text: str, lb_map: list[int]):\n",
                "    \"\"\"Восстанавливает переносы по карте\"\"\"\n",
                "    chars = list(text)\n",
                "    for pos in lb_map:\n",
                "        chars[pos] = \"\\n\"\n",
                "    return \"\".join(chars)\n",
                "\n",
                "\n",
                "# пример\n",
                "orig = \"раз два\\nтри четыре\\nпять\"\n",
                "print(\"ORIG:\", repr(orig))\n",
                "\n",
                "# заменили \\n на пробел\n",
                "prepared, lb_map = replacelinebreaks_with_spaces(orig)\n",
                "print(\"PREP:\", repr(prepared))\n",
                "print(\"MAP:\", lb_map)\n",
                "\n",
                "# ничего не делаем, просто восстанавливаем\n",
                "restored = restorelinebreaks(prepared, lb_map)\n",
                "print(\"RESTORED:\", repr(restored))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cae78cfe",
            "metadata": {},
            "source": [
                "!!! Возвращение переноса строки после обработки текста (карта по символам)   проверить"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "033f8aef",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from difflib import SequenceMatcher\n",
                "from typing import List, Callable\n",
                "\n",
                "def get_linebreak_map(text: str) -> List[int]:\n",
                "    return [i for i, ch in enumerate(text) if ch == \"\\n\"]\n",
                "\n",
                "def replacelinebreaks_with_spaces(text: str):\n",
                "    lb_map = get_linebreak_map(text)\n",
                "    # заменяем CRLF/CR на \\n заранее, если нужно:\n",
                "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
                "    prepared = text.replace('\\n', ' ')\n",
                "    return prepared, lb_map\n",
                "\n",
                "def map_orig_positions_to_processed(orig: str, proc: str, positions: List[int]) -> List[int]:\n",
                "    \"\"\"\n",
                "    Для каждой позиции из `positions` (индексы в orig) возвращает индекс\n",
                "    в proc, где лучше всего вставить/заменить на '\\n'.\n",
                "    \"\"\"\n",
                "    sm = SequenceMatcher(None, orig, proc)\n",
                "    opcodes = sm.get_opcodes()\n",
                "    positions_sorted = sorted(positions)\n",
                "    res = {}\n",
                "    pos_idx = 0\n",
                "\n",
                "    for tag, i1, i2, j1, j2 in opcodes:\n",
                "        # обрабатываем все позиции, попавшие в текущую область оригинала [i1,i2)\n",
                "        while pos_idx < len(positions_sorted) and positions_sorted[pos_idx] < i2:\n",
                "            pos = positions_sorted[pos_idx]\n",
                "            if pos < i1:\n",
                "                # на всякий случай\n",
                "                mapped = j1\n",
                "            else:\n",
                "                if tag == 'equal':\n",
                "                    mapped = j1 + (pos - i1)\n",
                "                elif tag == 'replace':\n",
                "                    # попытка сохранить относительное смещение, но не выходить за границы j1..j2\n",
                "                    rel = pos - i1\n",
                "                    if j2 > j1:\n",
                "                        mapped = j1 + min(rel, (j2 - j1 - 1))\n",
                "                    else:\n",
                "                        mapped = j1\n",
                "                elif tag == 'delete':\n",
                "                    # оригинал удалён — вставляем в точку j1 (перед следующими символами)\n",
                "                    mapped = j1\n",
                "                elif tag == 'insert':\n",
                "                    # сюда не попадём, потому что i1==i2 при insert\n",
                "                    mapped = j1\n",
                "                else:\n",
                "                    mapped = j1\n",
                "            res[pos] = mapped\n",
                "            pos_idx += 1\n",
                "            if pos_idx >= len(positions_sorted):\n",
                "                break\n",
                "        if pos_idx >= len(positions_sorted):\n",
                "            break\n",
                "\n",
                "    # если какие-то позиции остались (после конца), ставим в конец\n",
                "    while pos_idx < len(positions_sorted):\n",
                "        res[positions_sorted[pos_idx]] = len(proc)\n",
                "        pos_idx += 1\n",
                "\n",
                "    return [res[p] for p in positions]  # в исходном порядке\n",
                "\n",
                "def restorelinebreaks_by_mapped(proc: str, mapped_positions: List[int]) -> str:\n",
                "    \"\"\"\n",
                "    Восстановление: предпочитаем ЗАМЕНИТЬ пробел на '\\n' (на mapped index или рядом),\n",
                "    иначе вставляем '\\n' перед символом с индексом mapped.\n",
                "    \"\"\"\n",
                "    chars = list(proc)\n",
                "    n = len(chars)\n",
                "    insert_before = [False] * (n + 1)\n",
                "    replace_at = set()\n",
                "\n",
                "    # небольшое окно поиска пробела (чтобы корректно заменить, если пробел сдвинулся)\n",
                "    WINDOW = 3\n",
                "\n",
                "    for m in mapped_positions:\n",
                "        if m < 0:\n",
                "            m = 0\n",
                "        if m > n:\n",
                "            m = n\n",
                "\n",
                "        placed = False\n",
                "        # 1) попробуем заменить пробел точно на m\n",
                "        if m < n and chars[m] == ' ':\n",
                "            replace_at.add(m)\n",
                "            placed = True\n",
                "        # 2) попробуем ближайшие позиции (по возрастанию дистанции): m-1, m+1, m-2, m+2...\n",
                "        if not placed:\n",
                "            for d in range(1, WINDOW + 1):\n",
                "                left = m - d\n",
                "                right = m + d\n",
                "                if left >= 0 and left < n and chars[left] == ' ':\n",
                "                    replace_at.add(left)\n",
                "                    placed = True\n",
                "                    break\n",
                "                if right >= 0 and right < n and chars[right] == ' ':\n",
                "                    replace_at.add(right)\n",
                "                    placed = True\n",
                "                    break\n",
                "        # 3) если не нашли пробел — вставляем перед m\n",
                "        if not placed:\n",
                "            insert_before[m] = True\n",
                "\n",
                "    # собираем результат: учитываем замены и вставки\n",
                "    out = []\n",
                "    for i in range(n):\n",
                "        if insert_before[i]:\n",
                "            out.append('\\n')\n",
                "        if i in replace_at:\n",
                "            out.append('\\n')\n",
                "        else:\n",
                "            out.append(chars[i])\n",
                "    if insert_before[n]:\n",
                "        out.append('\\n')\n",
                "    return ''.join(out)\n",
                "\n",
                "def process_preservelinebreaks(text: str, processor: Callable[[str], str]) -> str:\n",
                "    \"\"\"\n",
                "    processor: функция, принимающая строку и возвращающая обработанную строку\n",
                "               (например, accentizer.process_all)\n",
                "    \"\"\"\n",
                "    prepared, lb_map = replacelinebreaks_with_spaces(text)\n",
                "    processed = processor(prepared)\n",
                "    mapped = map_orig_positions_to_processed(prepared, processed, lb_map)\n",
                "    restored = restorelinebreaks_by_mapped(processed, mapped)\n",
                "    return restored\n",
                "\n",
                "# ------------- пример -------------\n",
                "if __name__ == \"__main__\":\n",
                "    def fake_accentizer(s: str) -> str:\n",
                "        # тест: вставка '+' внутри слова \"жопа\"\n",
                "        return s.replace(\" \", \"   \")\n",
                "\n",
                "    orig = \"всё то что\\nзовётся жопа\\nи грусть\"\n",
                "    print(\"ORIG  :\", repr(orig))\n",
                "    result = process_preservelinebreaks(orig, fake_accentizer)\n",
                "    print(\"RESULT:\", repr(result))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3789815e",
            "metadata": {},
            "source": [
                "Простое удаление и восстановление переносов строки (карта по словам)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a9a7ca1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from typing import List, Callable\n",
                "\n",
                "def words_and_linebreak_map(text: str):\n",
                "    \"\"\"\n",
                "    Возвращает (words, lb_map)\n",
                "    words: список слов (переносы удалены)\n",
                "    lb_map: список позиций (число слов перед каждым переносом)\n",
                "    \"\"\"\n",
                "    # нормализуем окончания\n",
                "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
                "    # делаем \\n отдельным \"токеном\" и разделяем только по пробелу\n",
                "    parts = text.replace('\\n', ' \\n ').split(' ')\n",
                "    # сохраняем непустые токены, но оставляем '\\n'\n",
                "    tokens = [p for p in parts if p != '']\n",
                "    words = []\n",
                "    lb_map = []\n",
                "    wcount = 0\n",
                "    for t in tokens:\n",
                "        if t == '\\n':\n",
                "            # перенос после wcount слов\n",
                "            lb_map.append(wcount)\n",
                "        else:\n",
                "            words.append(t)\n",
                "            wcount += 1\n",
                "    return words, lb_map\n",
                "\n",
                "def restore_words_withlinebreaks(words: List[str], lb_map: List[int]) -> str:\n",
                "    \"\"\"\n",
                "    Вставляет переносы по карте и соединяет слова в строку.\n",
                "    (Сжимает последовательности пробелов до одиночных.)\n",
                "    \"\"\"\n",
                "    lb_set = set(lb_map)\n",
                "    out_tokens = []\n",
                "    # переносы в начале (если есть lb_map с 0)\n",
                "    if 0 in lb_set:\n",
                "        out_tokens.append('\\n')\n",
                "    for i, w in enumerate(words):\n",
                "        out_tokens.append(w)\n",
                "        # если после (i+1)-го слова нужно перенос — добавляем\n",
                "        if (i + 1) in lb_set:\n",
                "            out_tokens.append('\\n')\n",
                "    # склеиваем\n",
                "    s = ' '.join(out_tokens)\n",
                "    # заменим \" <newline> \" на настоящий перенос\n",
                "    return s.replace(' \\n ', '\\n')\n",
                "\n",
                "def process_text_wordwise(text: str, word_processor: Callable[[str], str]):\n",
                "    \"\"\"\n",
                "    Вход: исходный текст и функция word_processor(word)->processed_word.\n",
                "    Работает по словам, возвращает текст с восстановленными переносами.\n",
                "    \"\"\"\n",
                "    words, lb_map = words_and_linebreak_map(text)\n",
                "    # обработка слова за словом (сохраняется 1:1 соответствие)\n",
                "    processed = [word_processor(w) for w in words]\n",
                "    return restore_words_withlinebreaks(processed, lb_map)\n",
                "\n",
                "# ---------- пример использования ----------\n",
                "if __name__ == \"__main__\":\n",
                "    orig = \"\"\"в иллюминатор     постучали\n",
                "и изменившийся в лице\n",
                "гагарин ищет по карманам\n",
                "кэ цэ\n",
                "\"\"\"\n",
                "    print(\"ORIG:  \", repr(orig))\n",
                "\n",
                "    # эмуляция процессора: ставит ударение в одном слове\n",
                "    def fake_proc(w):\n",
                "        return w.replace(\"лице\", \"лиц+е\")\n",
                "\n",
                "    out = process_text_wordwise(orig, fake_proc)\n",
                "    print(\"RESULT:\", repr(out))\n",
                "    # ожидаем: 'всё то что\\nзовётся ж+опа\\nи грусть'\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd8b094f",
            "metadata": {},
            "source": [
                "Возвращение переноса строки после обработки текста (карта по словам)   показана только карта       полная функция ниже и в linebreaks.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2572d2d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def update_linebreak_map(word_map, changes):\n",
                "    \"\"\"\n",
                "    Корректирует карту переносов при изменении числа слов.\n",
                "\n",
                "    word_map: список индексов переносов (например [4, 7, 10])\n",
                "    changes: список кортежей (pos, old_count, new_count)\n",
                "             pos       – индекс слова в исходном тексте (где произошла замена)\n",
                "             old_count – сколько слов было\n",
                "             new_count – сколько слов стало\n",
                "    \"\"\"\n",
                "    # сортируем изменения по позиции, чтобы применять по порядку\n",
                "    changes = sorted(changes, key=lambda x: x[0])\n",
                "    new_map = word_map[:]\n",
                "    shift = 0\n",
                "\n",
                "    for pos, old, new in changes:\n",
                "        delta = new - old\n",
                "        # все переносы, идущие ПОСЛЕ или НА этой позиции, двигаются\n",
                "        new_map = [idx + delta if idx >= pos + shift else idx for idx in new_map]\n",
                "        shift += delta\n",
                "\n",
                "    return new_map\n",
                "\n",
                "word_map = [4, 7, 10]\n",
                "changes = [(5, 2, 1)]\n",
                "\n",
                "print(update_linebreak_map(word_map, changes))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb5c55a8",
            "metadata": {},
            "source": [
                "!!! Возвращение переноса строки после обработки текста (карта по словам)   проверить       см в linebreaks.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "90bb9a97",
            "metadata": {},
            "outputs": [],
            "source": [
                "import difflib\n",
                "from collections import Counter\n",
                "from typing import List, Tuple, Callable\n",
                "\n",
                "def words_and_map(text: str) -> Tuple[List[str], List[int]]:\n",
                "    \"\"\"\n",
                "    Возвращает (words, word_map).\n",
                "    word_map содержит позиции (число слов до переноса), например [3, 6].\n",
                "    Сохраняет переносы как отдельный токен '\\n'.\n",
                "    \"\"\"\n",
                "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
                "    # делаем '\\n' отдельным токеном, затем split по одиночному пробелу,\n",
                "    # чтобы не терять маркеры '\\n'\n",
                "    spaced = text.replace('\\n', ' \\n ')\n",
                "    parts = spaced.split(' ')  # split(' ') сохраняет пустые элементы для множественных пробелов\n",
                "    tokens = [p for p in parts if p != '']  # удаляем пустые элементы, но сохраняем '\\n'\n",
                "    words = []\n",
                "    word_map = []\n",
                "    wcount = 0\n",
                "    for t in tokens:\n",
                "        if t == '\\n':\n",
                "            # перенос после wcount слов (то есть между wcount-1 и wcount)\n",
                "            word_map.append(wcount)\n",
                "        else:\n",
                "            words.append(t)\n",
                "            wcount += 1\n",
                "    return words, word_map\n",
                "\n",
                "def _ensure_list_words(out) -> List[str]:\n",
                "    \"\"\"Нормализует результат процессора в список слов.\"\"\"\n",
                "    if isinstance(out, list):\n",
                "        return out\n",
                "    if isinstance(out, str):\n",
                "        return out.split()\n",
                "    # любая итерируемая последовательность\n",
                "    return list(out)\n",
                "\n",
                "def try_process(before_words: List[str], processor: Callable) -> List[str]:\n",
                "    \"\"\"\n",
                "    Попытка вызвать processor двумя способами:\n",
                "      1) processor(before_words) — если процессор умеет работать с list\n",
                "      2) processor(' '.join(before_words)) — если процессор ожидает строку\n",
                "    Возвращает список слов (после .split()).\n",
                "    \"\"\"\n",
                "    try:\n",
                "        out = processor(before_words)\n",
                "    except TypeError:\n",
                "        out = processor(' '.join(before_words))\n",
                "    return _ensure_list_words(out)\n",
                "\n",
                "def map_boundaries(before: List[str], after: List[str], positions: List[int]) -> List[int]:\n",
                "    \"\"\"\n",
                "    Для каждого boundary в positions (0..len(before)) возвращает позицию в after,\n",
                "    означающую сколько слов идёт перед соответствующим переносом в обработанном тексте.\n",
                "    Использует opcodes SequenceMatcher для корректного сопоставления блоков.\n",
                "    \"\"\"\n",
                "    sm = difflib.SequenceMatcher(a=before, b=after)\n",
                "    opcodes = sm.get_opcodes()\n",
                "    mapped = []\n",
                "    for pos in positions:\n",
                "        m = 0\n",
                "        for tag, i1, i2, j1, j2 in opcodes:\n",
                "            if pos >= i2:\n",
                "                # целиком лежит до границы — добавляем весь блок after-length\n",
                "                m += (j2 - j1)\n",
                "                continue\n",
                "            if pos <= i1:\n",
                "                # граница лежит до начала этого блока — больше ничего не добавляем\n",
                "                break\n",
                "            # граница внутри текущего блока: i1 < pos < i2\n",
                "            # если блок равный, можно точно добавить соответствующую часть\n",
                "            if tag == 'equal':\n",
                "                m += (pos - i1)\n",
                "            else:\n",
                "                # если это replace/delete/insert — разумный выбор:\n",
                "                # считаем, что граница映ируется в начало соответствующего блока after (j1)\n",
                "                # (альтернативы: j1 + min(pos-i1, j2-j1) — но это делает поведение менее детерминированным)\n",
                "                m += 0\n",
                "            break\n",
                "        mapped.append(m)\n",
                "    return mapped\n",
                "\n",
                "#  ???  а если сделать универсальный вход - (list или str) \n",
                "def restorelinebreaks_from_words(words: List[str], word_map: List[int]) -> str:\n",
                "    \"\"\"\n",
                "    Склеивает список слов в строку, вставляя '\\n' согласно word_map (число слов до переноса).\n",
                "    Поддерживает несколько переносов подряд (повторы позиции).\n",
                "    \"\"\"\n",
                "    counts = Counter(word_map)\n",
                "    lines = []\n",
                "    # переносы в начале (если есть позиции 0)\n",
                "    for _ in range(counts.get(0, 0)):\n",
                "        lines.append('')\n",
                "    current = []\n",
                "    for i, w in enumerate(words, start=1):\n",
                "        current.append(w)\n",
                "        c = counts.get(i, 0)\n",
                "        if c:\n",
                "            lines.append(' '.join(current))\n",
                "            for _ in range(c - 1):\n",
                "                lines.append('')  # пустые строки при последовательных переносах\n",
                "            current = []\n",
                "    if current:\n",
                "        lines.append(' '.join(current))\n",
                "    return '\\n'.join(lines)\n",
                "\n",
                "def linebreaks_restore(text: str, processor: Callable) -> Tuple[str, dict]:\n",
                "    \"\"\"\n",
                "    Универсальная обёртка:\n",
                "      - строит карту переносов по словам,\n",
                "      - пропускает через processor (list->list или str->str),\n",
                "      - строит изменения / маппинг границ и восстанавливает '\\n'.\n",
                "    Возвращает (restored_text, debug_dict)\n",
                "    \"\"\"\n",
                "    before_words, old_map = words_and_map(text)\n",
                "    after_words = try_process(before_words, processor)\n",
                "    new_map = map_boundaries(before_words, after_words, old_map)\n",
                "    restored = restorelinebreaks_from_words(after_words, new_map)\n",
                "    debug = {\n",
                "        'before_words': before_words,\n",
                "        'after_words': after_words,\n",
                "        'old_map': old_map,\n",
                "        'new_map': new_map,\n",
                "    }\n",
                "    return restored, debug\n",
                "\n",
                "# ========== пример ==========\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    text = \"всё то что\\n125 зовётся жопа\\nи грусть\"\n",
                "\n",
                "    def fake_numberizer(tokens):\n",
                "        out = []\n",
                "        for t in tokens:\n",
                "            if t == \"125\":\n",
                "                out.extend([\"сто\", \"двадцать\", \"пять\"])\n",
                "            else:\n",
                "                out.append(t)\n",
                "        return out\n",
                "\n",
                "    restored, dbg = linebreaks_restore(text, fake_numberizer)\n",
                "    print(\"ORIG:\", repr(text))\n",
                "    print(\"BEFORE:\", dbg['before_words'])\n",
                "    print(\"OLD_MAP:\", dbg['old_map'])\n",
                "    print(\"AFTER:\", dbg['after_words'])\n",
                "    print(\"NEW_MAP:\", dbg['new_map'])\n",
                "    print(\"RESTORED:\\n\" + restored)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f1188bcb",
            "metadata": {},
            "source": [
                "==========================================================="
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e4307e1e",
            "metadata": {},
            "source": [
                "пунктуация (открывающая и закрывающая) корректное возвращение мз списка в строку"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "86f9923d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(привет, друг!) ля дя!\n"
                    ]
                }
            ],
            "source": [
                "from typing import List\n",
                "\n",
                "CLOSE_PUNCT = \".,!?;:)]}»\"\n",
                "OPEN_PUNCT  = \"([{\\\"«\"\n",
                "\n",
                "def detokenize(tokens: List[str]) -> str:\n",
                "    out = []\n",
                "    for i, tok in enumerate(tokens):\n",
                "        if i == 0:\n",
                "            out.append(tok)\n",
                "            continue\n",
                "\n",
                "        prev = tokens[i-1]\n",
                "\n",
                "        if tok in CLOSE_PUNCT:\n",
                "            # слева без пробела\n",
                "            out[-1] = out[-1] + tok\n",
                "        elif prev in OPEN_PUNCT:\n",
                "            # справа без пробела\n",
                "            out[-1] = out[-1] + tok\n",
                "        else:\n",
                "            # обычный случай\n",
                "            out.append(\" \" + tok)\n",
                "    return \"\".join(out)\n",
                "\n",
                "\n",
                "# пример\n",
                "print(detokenize([\"(\", \"привет\", \",\", \"друг\", \"!\", \")\", \"ля дя\", \"!\"]))\n",
                "# -> \"(привет, друг)\"\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
