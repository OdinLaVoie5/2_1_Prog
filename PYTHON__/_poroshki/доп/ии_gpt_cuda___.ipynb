{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d9c464",
   "metadata": {},
   "source": [
    "___ НЕЙРОСЕТИ ___ ИИ ___  МОДЕЛИ  ___  GPU  и  CPU ___ CUDA ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad486f1",
   "metadata": {},
   "source": [
    "Токены  и  id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 1. Загружаем токенизатор и модель\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 2. Текст для теста\n",
    "text = \"Привет, мир!\"\n",
    "\n",
    "# 3. Токенизация\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Токены:\", tokens)\n",
    "print(\"ID токенов:\", token_ids)\n",
    "\n",
    "# 4. Генерация текста\n",
    "inputs = tokenizer.encode(\"Жил-был кот,\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=30, do_sample=True, top_k=40)\n",
    "\n",
    "print(\"Сгенерированный текст:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "#  Функция для показа токенов (ID и текст) (а то в 3. кракозябры)\n",
    "def show_tokens(text: str):\n",
    "    print(\"Текст:\", text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    for i, (tid, tok) in enumerate(zip(token_ids, tokens)):\n",
    "        # сразу нормальный вид токена через decode\n",
    "        print(\n",
    "            f\"{i}: {tid:<6} → {tokenizer.decode([tid])!r} (сырой токен: {tok})\")\n",
    "\n",
    "\n",
    "# пример\n",
    "show_tokens(\"Привет, мир!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fef5d",
   "metadata": {},
   "source": [
    "Токены  и  id   +  !!!  GPT (рус)  РАЗНЫЕ МОДЕЛИ (некоторые занимают много места  и  жрут ресурсы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import textwrap\n",
    "\n",
    "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "\"\"\" sberbank-ai/rugpt3small_based_on_gpt2 ??\n",
    "         другие модели (дольше!)  \n",
    "    ai-forever/rugpt3large_based_on_gpt2 \n",
    "    ai-forever/rugpt3medium_on_gpt2  ??\n",
    "    ai-forever/rugpt3xl_based_on_gpt2 \n",
    "    !!!   ai-forever/mGPT \n",
    "    cointegrated/rut5-base\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# автоматически выбираем устройство\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Используем устройство: {device}\")\n",
    "\n",
    "text = \"Привет, мир!\"\n",
    "\n",
    "# перемещаем входные данные на то же устройство\n",
    "inputs = tokenizer(\"Когда меня ты позовёшь\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], max_length=500, do_sample=True, top_k=40\n",
    ")\n",
    "\n",
    "novell = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "wrapped = textwrap.fill(novell, width=60)\n",
    "print(wrapped + \"\\n\")\n",
    "\n",
    "# ========================\n",
    "# Функция для показа токенов\n",
    "\n",
    "\n",
    "def show_tokens(text: str):\n",
    "    print(\"Текст:\", text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    for i, (tid, tok) in enumerate(zip(token_ids, tokens)):\n",
    "        print(\n",
    "            f\"{i}: {tid:<6} → {tokenizer.decode([tid])!r} (сырой токен: {tok})\")\n",
    "\n",
    "\n",
    "# пример\n",
    "show_tokens(\"экспроприатор', архиважнейшее подвзбзднул!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477d91e",
   "metadata": {},
   "source": [
    "Инфо о CUDA  GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "\n",
    "print(torch.cuda.is_available())   # True = всё ок\n",
    "print(torch.cuda.get_device_name(0))  # название карты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18734b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Проверка: видит ли CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Создадим тензор на GPU и прогонем простую операцию\n",
    "x = torch.randn(10000, 10000, device=\"cuda\")   # создаём тензор сразу на GPU\n",
    "y = torch.mm(x, x)                             # матричное умножение (нагрузка)\n",
    "print(\"Результат:\", y[0, 0].item())\n",
    "print(\"Тензор хранится на:\", y.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2722",
   "metadata": {},
   "source": [
    "ннннн  У меня pytorch и cuda вместе работают! ^ \n",
    "А onnxruntime и cuda вместе    не работают!   v\n",
    "(нужен ли? 1 https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=Server2025&target_type=exe_local) \n",
    "2  https://developer.nvidia.com/cudnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e52d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Все доступные в сборке onnxruntime провайдеры\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "\n",
    "# Пытаемся создать сессию с CUDA         \n",
    "session = ort.InferenceSession(\n",
    "    r\"C:\\Users\\Odins\\new-git\\2_1_Prog\\models\\model.onnx\",\n",
    "    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "print(\"Session providers:\", session.get_providers())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f9c50",
   "metadata": {},
   "source": [
    "Проверка версий torch ((обучатель)) и cuda (ускоритель, gpu)  + gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda runtime:\", torch.version.cuda)\n",
    "    !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7b748",
   "metadata": {},
   "source": [
    "инфо коротко  torch и onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1566a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(\n",
    "    0) if torch.cuda.is_available() else \"none\")\n",
    "print(\"onnxruntime providers:\", ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e58af",
   "metadata": {},
   "source": [
    "onnxruntime версия "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a22a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "print(\"onnxruntime ver:\", ort.__version__)\n",
    "print(\"available providers:\", ort.get_available_providers())\n",
    "print(\"default provider:\", ort.get_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b776d7a",
   "metadata": {},
   "source": [
    "Проверка поддержки FP16 и INT8.   + Тип провайдеров ((рассчётчиков тензоров для моделей))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CUDA available (torch):\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(\"GPU:\", device_name)\n",
    "    print(\"Compute capability:\", capability)\n",
    "\n",
    "    major, minor = capability\n",
    "\n",
    "    # FP16\n",
    "    if major >= 5:  # Pascal (5.3) и выше\n",
    "        print(\"✅ FP16 поддерживается\")\n",
    "    else:\n",
    "        print(\"❌ FP16 не поддерживается\")\n",
    "\n",
    "    # INT8\n",
    "    if major >= 7:  # Turing (7.0) и выше\n",
    "        print(\"✅ INT8 поддерживается\")\n",
    "    else:\n",
    "        print(\"❌ INT8 не поддерживается (будет очень медленно или совсем не работать)\")\n",
    "print(\"=\"*70)\n",
    "print(\"ONNX Runtime info:\")\n",
    "print(\"onnxruntime ver:\", ort.__version__)\n",
    "print(\"available providers:\", ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adf8d2",
   "metadata": {},
   "source": [
    "Тест  - сравнение быстроты  gpu  и  cpu в задачах ии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "# ===== Настройки =====\n",
    "prompt_text = \"Жил-был кот,\"\n",
    "max_length = 200\n",
    "top_k = 40\n",
    "wrap_width = 60\n",
    "\n",
    "# ===== Функция генерации с таймером =====\n",
    "def generate_text(model, tokenizer, inputs, max_length=160, top_k=40):\n",
    "    start = time.time()\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text, elapsed\n",
    "\n",
    " \n",
    "# ===== Загрузка токенизатора и модели =====\n",
    "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# ===== Подготовка входа =====\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# ===== Проверка GPU =====\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    print(f\"GPU найден: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU не найден, используем CPU\")\n",
    "\n",
    "# ===== Генерация на GPU (если есть) =====\n",
    "if gpu_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    inputs_cuda = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    text_gpu, time_gpu = generate_text(\n",
    "        model, tokenizer, inputs_cuda, max_length, top_k)\n",
    "    print(\"\\n--- GPU ---\")\n",
    "    print(f\"Время генерации: {time_gpu:.3f} сек\")\n",
    "    print(textwrap.fill(text_gpu, width=wrap_width))\n",
    "\n",
    "# ===== Генерация на CPU =====\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "inputs_cpu = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "text_cpu, time_cpu = generate_text(\n",
    "    model, tokenizer, inputs_cpu, max_length, top_k)\n",
    "print(\"\\n--- CPU ---\")\n",
    "print(f\"Время генерации: {time_cpu:.3f} сек\")\n",
    "print(textwrap.fill(text_cpu, width=wrap_width))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec19666",
   "metadata": {},
   "source": [
    "Тест   -  сравнение быстроты  gpu  и  cpu  (ПРОДВИНУТЫЙ) !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cccbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "bench_generate.py\n",
    "Честный бенчмарк генерации HuggingFace (GPU vs CPU) с защитой по версиям.\n",
    "Копировать/вставить и запускать.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "import statistics\n",
    "import inspect\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -------------------------- Настройки --------------------------\n",
    "MODEL_NAME = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "PROMPT = \"Жил-был кот,\"\n",
    "MAX_NEW_TOKENS = 180\n",
    "TOP_K = 40\n",
    "WRAP_WIDTH = 70\n",
    "\n",
    "WARMUP_RUNS = 2\n",
    "MEASURE_RUNS = 5\n",
    "SEED = 42\n",
    "\n",
    "# Режим: \"sequential\" (по умолчанию, безопасный) или \"parallel\" (требует много памяти)\n",
    "RUN_MODE = \"sequential\"\n",
    "\n",
    "# Для GPU: использовать fp16 (half) — да/нет\n",
    "USE_FP16_ON_GPU = True\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "def print_sep():\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def info_versions():\n",
    "    import transformers\n",
    "    import tokenizers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    try:\n",
    "        print(\"tokenizers: \", tokenizers.__version__)\n",
    "    except Exception:\n",
    "        print(\"tokenizers: <unknown>\")\n",
    "    print(\"torch:      \", torch.__version__)\n",
    "    print(\"cuda avail: \", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda ver:   \", torch.version.cuda)\n",
    "\n",
    "\n",
    "def load_tokenizer(model_name=MODEL_NAME):\n",
    "    print(\"Загрузка токенизатора...\")\n",
    "    return AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def load_model_on_device(model_name=MODEL_NAME, device=torch.device(\"cpu\"), use_fp16=False):\n",
    "    print(\n",
    "        f\"Загрузка модели '{model_name}' на устройство {device} (fp16={use_fp16})...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    try:\n",
    "        if device.type == \"cuda\":\n",
    "            if use_fp16:\n",
    "                model.half().to(device)\n",
    "            else:\n",
    "                model.to(device)\n",
    "        else:\n",
    "            model.to(device)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Предупреждение: не удалось применить fp16 / переместить модель как хотели:\", e)\n",
    "        model.to(device)  # попытка в любом случае\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_inputs(tokenizer, prompt, device):\n",
    "    t = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return {k: v.to(device) for k, v in t.items()}\n",
    "\n",
    "\n",
    "def make_generator(seed: int, device: torch.device):\n",
    "    try:\n",
    "        gen = torch.Generator(device=device).manual_seed(seed)\n",
    "    except Exception:\n",
    "        gen = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    return gen\n",
    "\n",
    "\n",
    "def generate_with_fallback(model, tokenizer, inputs, device, max_new_tokens, top_k, seed=None):\n",
    "    \"\"\"\n",
    "    Корректный вызов model.generate с защитой от разных версий transformers:\n",
    "    - если model.generate поддерживает аргумент 'generator' (см. inspect.signature) -> используем его;\n",
    "    - иначе ставим глобальные сиды (torch.manual_seed / torch.cuda.manual_seed_all).\n",
    "    Всегда синхронизируем CUDA до/после для корректного тайминга.\n",
    "    \"\"\"\n",
    "    # параметры генерации\n",
    "    gen_kwargs = dict(max_new_tokens=max_new_tokens,\n",
    "                      do_sample=True, top_k=top_k)\n",
    "\n",
    "    # проверяем сигнатуру\n",
    "    try:\n",
    "        sig = inspect.signature(model.generate)\n",
    "        accepts_generator = 'generator' in sig.parameters\n",
    "    except Exception:\n",
    "        accepts_generator = False\n",
    "\n",
    "    # подготовка генератора если надо\n",
    "    if seed is not None and accepts_generator:\n",
    "        try:\n",
    "            generator = make_generator(\n",
    "                seed, device if device.type == \"cuda\" else torch.device(\"cpu\"))\n",
    "            gen_kwargs[\"generator\"] = generator\n",
    "        except Exception:\n",
    "            # fallback: не добавляем generator, будем ставить глобальные сиды\n",
    "            gen_kwargs.pop(\"generator\", None)\n",
    "\n",
    "    # если generator не используется - ставим глобальные сиды\n",
    "    if seed is not None and \"generator\" not in gen_kwargs:\n",
    "        torch.manual_seed(seed)\n",
    "        if device.type == \"cuda\":\n",
    "            try:\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # синхронизация перед измерением\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.inference_mode():\n",
    "        # explicit input_ids and attention_mask\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs.get(\"attention_mask\", None),\n",
    "            **gen_kwargs\n",
    "        )\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text, elapsed\n",
    "\n",
    "\n",
    "def run_benchmark_on_device(model, tokenizer, device, prompt, warmup=WARMUP_RUNS, runs=MEASURE_RUNS, seed=SEED):\n",
    "    print(f\"\\nЗапуск на устройстве: {device} (warmup={warmup}, runs={runs})\")\n",
    "    inputs = prepare_inputs(tokenizer, prompt, device)\n",
    "\n",
    "    # warm-up\n",
    "    for i in range(warmup):\n",
    "        _ = generate_with_fallback(\n",
    "            model, tokenizer, inputs, device, MAX_NEW_TOKENS, TOP_K, seed=seed + i)\n",
    "\n",
    "    times = []\n",
    "    texts = []\n",
    "    for i in range(runs):\n",
    "        out, t = generate_with_fallback(\n",
    "            model, tokenizer, inputs, device, MAX_NEW_TOKENS, TOP_K, seed=seed + i)\n",
    "        times.append(t)\n",
    "        texts.append(out)\n",
    "        print(f\"  run {i+1}/{runs}: {t:.3f}s\")\n",
    "\n",
    "    mean_t = statistics.mean(times)\n",
    "    std_t = statistics.stdev(times) if len(times) > 1 else 0.0\n",
    "    print(f\"Результат: mean={mean_t:.3f}s std={std_t:.3f}s\")\n",
    "    return texts, times\n",
    "\n",
    "\n",
    "def safe_unload_model(model):\n",
    "    try:\n",
    "        del model\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    print_sep()\n",
    "    info_versions()\n",
    "    print_sep()\n",
    "\n",
    "    tokenizer = load_tokenizer(MODEL_NAME)\n",
    "\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    device_gpu = torch.device(\"cuda\") if gpu_available else None\n",
    "\n",
    "    # parallel mode\n",
    "    if RUN_MODE == \"parallel\" and gpu_available:\n",
    "        try:\n",
    "            model_gpu = load_model_on_device(\n",
    "                MODEL_NAME, device_gpu, use_fp16=USE_FP16_ON_GPU)\n",
    "            model_cpu = load_model_on_device(\n",
    "                MODEL_NAME, torch.device(\"cpu\"), use_fp16=False)\n",
    "            texts_gpu, times_gpu = run_benchmark_on_device(\n",
    "                model_gpu, tokenizer, device_gpu, PROMPT)\n",
    "            texts_cpu, times_cpu = run_benchmark_on_device(\n",
    "                model_cpu, tokenizer, torch.device(\"cpu\"), PROMPT)\n",
    "            print_sep()\n",
    "            print(\"GPU final text:\")\n",
    "            print(textwrap.fill(texts_gpu[-1], width=WRAP_WIDTH))\n",
    "            print_sep()\n",
    "            print(\"CPU final text:\")\n",
    "            print(textwrap.fill(texts_cpu[-1], width=WRAP_WIDTH))\n",
    "            safe_unload_model(model_gpu)\n",
    "            safe_unload_model(model_cpu)\n",
    "            return\n",
    "        except RuntimeError as e:\n",
    "            print(\n",
    "                \"Ошибка при параллельной загрузке (вероятно OOM), переключаемся в sequential:\", e)\n",
    "\n",
    "    # sequential (default)\n",
    "    texts_gpu, times_gpu = [], []\n",
    "    if gpu_available:\n",
    "        try:\n",
    "            model_gpu = load_model_on_device(\n",
    "                MODEL_NAME, device_gpu, use_fp16=USE_FP16_ON_GPU)\n",
    "            texts_gpu, times_gpu = run_benchmark_on_device(\n",
    "                model_gpu, tokenizer, device_gpu, PROMPT)\n",
    "            print_sep()\n",
    "            print(\"GPU final text:\")\n",
    "            print(textwrap.fill(texts_gpu[-1], width=WRAP_WIDTH))\n",
    "            safe_unload_model(model_gpu)\n",
    "        except Exception as e:\n",
    "            print(\"Ошибка при запуске на GPU:\", e)\n",
    "            texts_gpu, times_gpu = [], []\n",
    "\n",
    "    # CPU\n",
    "    try:\n",
    "        model_cpu = load_model_on_device(\n",
    "            MODEL_NAME, torch.device(\"cpu\"), use_fp16=False)\n",
    "        texts_cpu, times_cpu = run_benchmark_on_device(\n",
    "            model_cpu, tokenizer, torch.device(\"cpu\"), PROMPT)\n",
    "        print_sep()\n",
    "        print(\"CPU final text:\")\n",
    "        print(textwrap.fill(texts_cpu[-1], width=WRAP_WIDTH))\n",
    "        safe_unload_model(model_cpu)\n",
    "    except Exception as e:\n",
    "        print(\"Ошибка при запуске на CPU:\", e)\n",
    "        texts_cpu, times_cpu = [], []\n",
    "\n",
    "    # Summary\n",
    "    print_sep()\n",
    "    if times_gpu:\n",
    "        print(\n",
    "            f\"GPU mean: {statistics.mean(times_gpu):.3f}s (runs={len(times_gpu)})\")\n",
    "    if times_cpu:\n",
    "        print(\n",
    "            f\"CPU mean: {statistics.mean(times_cpu):.3f}s (runs={len(times_cpu)})\")\n",
    "    print_sep()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34aa7a",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee42f8",
   "metadata": {},
   "source": [
    "НАХОДИТ ПАПКИ С МОДЕЛЯМИ  ИИ   C:\\Users\\Odins\\.cache\\huggingface\\hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eef66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def get_size(path: pathlib.Path) -> int:\n",
    "    \"\"\"Размер папки в байтах\"\"\"\n",
    "    total = 0\n",
    "    for p in path.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            total += p.stat().st_size\n",
    "    return total\n",
    "\n",
    "\n",
    "def find_huggingface_models():\n",
    "    # Стандартные пути\n",
    "    candidates = []\n",
    "    home = pathlib.Path.home()\n",
    "    hf_home = os.getenv(\"HF_HOME\")\n",
    "\n",
    "    if hf_home:\n",
    "        candidates.append(pathlib.Path(hf_home))\n",
    "    candidates.append(home / \".cache\" / \"huggingface\" / \"transformers\")\n",
    "    candidates.append(home / \".cache\" / \"huggingface\" / \"hub\")\n",
    "\n",
    "    results = []\n",
    "    for base in candidates:\n",
    "        if base.exists():\n",
    "            for p in base.rglob(\"*\"):\n",
    "                if p.is_dir():\n",
    "                    # ищем модельные папки (обычно содержат config.json или pytorch_model.bin)\n",
    "                    if any((p / fname).exists() for fname in [\"config.json\", \"pytorch_model.bin\", \"generation_config.json\"]):\n",
    "                        size_mb = get_size(p) / (1024 * 1024)\n",
    "                        results.append((str(p), round(size_mb, 2)))\n",
    "    return results\n",
    "\n",
    "\n",
    "models = find_huggingface_models()\n",
    "if not models:\n",
    "    print(\"❌ Модели HuggingFace не найдены!\")\n",
    "else:\n",
    "    print(\"✅ Найдены модели:\")\n",
    "    for path, size in models:\n",
    "        print(f\"{path} — {size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6d015",
   "metadata": {},
   "source": [
    "МНОГО МОДЕЛЕЙ В ОДНОМ КОДЕ (поочерёдно)\n",
    "models = [\n",
    "    \"cointegrated/rut5-base-multitask\",\n",
    "    \"ai-forever/ruT5-base\",\n",
    "    \"sberbank-ai/ruT5-base\"\n",
    "]\n",
    "\n",
    "for m in models:\n",
    "    tok, model = load_model(m)\n",
    "    # тут работа с моделью\n",
    "    text = \"Привет, я пошол дамой\"\n",
    "    inputs = tok(text, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_new_tokens=30)\n",
    "    print(tok.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "    # выгружаем\n",
    "    del tok, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "!!!   Так можно перебирать хоть десятки моделей, не держа их все в памяти сразу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852fe49",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc502208",
   "metadata": {},
   "source": [
    "пример gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# 1. Загружаем токенизатор и модель\n",
    "MODEL_NAME = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# GPT2 не имеет token_type_ids → отключим\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 2. Датасет (пример)\n",
    "texts = [\n",
    "    \"и вот настал рассвет\",\n",
    "    \"вечером у костра сидели рыбаки\",\n",
    "    \"над степью летела стая журавлей\",\n",
    "]\n",
    "\n",
    "encodings = tokenizer(texts, truncation=True,\n",
    "                      padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Класс датасета\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx])\n",
    "                for key, val in self.encodings.items()}\n",
    "        # ВАЖНО: labels нужны для Trainer (иначе ошибка ValueError)\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "\n",
    "dataset = TextDataset(encodings)\n",
    "\n",
    "# 4. Аргументы тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    fp16=torch.cuda.is_available(),  # если GPU умеет FP16\n",
    ")\n",
    "\n",
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# 6. Запуск обучения\n",
    "trainer.train()\n",
    "\n",
    "# 7. Проверка генерации текста\n",
    "prompt = \"и вот настала ночь\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c60b78",
   "metadata": {},
   "source": [
    "пример gpt  -  генерации текста (разные стили)  -  temperature ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Загружаем модель один раз\n",
    "MODEL_NAME = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# GPT2 не имеет pad_token → ставим eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# --- функция генерации с выбором стиля ---\n",
    "def generate_text(prompt: str,\n",
    "                  style: str = \"neutral\",\n",
    "                  max_new_tokens: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    Генерация текста с творческим стилем\n",
    "    style:\n",
    "        'neutral'   – обычный текст\n",
    "        'poetic'    – поэтический, лирический\n",
    "        'descriptive' – описательный, живописный\n",
    "        'dialogue'  – диалоговый, разговорный\n",
    "    \"\"\"\n",
    "\n",
    "    # задаём параметры генерации по стилю\n",
    "    style_params = {\n",
    "        \"neutral\":       {\"temperature\": 1.0, \"top_k\": 50, \"top_p\": 0.95, \"repetition_penalty\": 1.2},\n",
    "        \"poetic\":        {\"temperature\": 1.2, \"top_k\": 100, \"top_p\": 0.98, \"repetition_penalty\": 1.1},\n",
    "        \"descriptive\":   {\"temperature\": 1.1, \"top_k\": 80, \"top_p\": 0.95, \"repetition_penalty\": 1.2},\n",
    "        \"dialogue\":      {\"temperature\": 1.3, \"top_k\": 120, \"top_p\": 0.98, \"repetition_penalty\": 1.0},\n",
    "    }\n",
    "\n",
    "    params = style_params.get(style, style_params[\"neutral\"])\n",
    "\n",
    "    # можно слегка подправить промпт для стиля\n",
    "    if style == \"poetic\":\n",
    "        prompt = f\"{prompt}\\n—\"\n",
    "    elif style == \"descriptive\":\n",
    "        prompt = f\"{prompt}. \"\n",
    "    elif style == \"dialogue\":\n",
    "        prompt = f'\"{prompt}\" — '\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=params[\"temperature\"],\n",
    "        top_k=params[\"top_k\"],\n",
    "        top_p=params[\"top_p\"],\n",
    "        repetition_penalty=params[\"repetition_penalty\"]\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# --- Примеры ---\n",
    "prompt = \"и вот настала ночь\"\n",
    "\n",
    "print(\"--- нейтральный ---\")\n",
    "print(generate_text(prompt, style=\"neutral\"))\n",
    "\n",
    "print(\"\\n--- поэтический ---\")\n",
    "print(generate_text(prompt, style=\"poetic\"))\n",
    "\n",
    "print(\"\\n--- описательный ---\")\n",
    "print(generate_text(prompt, style=\"descriptive\"))\n",
    "\n",
    "print(\"\\n--- диалоговый ---\")\n",
    "print(generate_text(prompt, style=\"dialogue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245286f",
   "metadata": {},
   "source": [
    "пример gpt - (наказание за повторы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"и вот настала ночь\"\n",
    "max_tokens = 60 \n",
    "\n",
    "for rp in [1.0, 1.2, 1.5]:\n",
    "    print(f\"\\n--- repetition_penalty = {rp} ---\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=rp\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dceb94",
   "metadata": {},
   "source": [
    "==================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
